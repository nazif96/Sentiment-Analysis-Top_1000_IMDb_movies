{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b0c55925",
   "metadata": {},
   "source": [
    "## **Deep Learning avec LSTM (mémoire à long terme) :**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b46bc074",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\NAZIFOU\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "import os  \n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns \n",
    "\n",
    "from pathlib import Path\n",
    "sns.set_theme(style=\"whitegrid\", palette=\"pastel\")\n",
    "from matplotlib import rcParams\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords') \n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer \n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d52efa26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Movie Name</th>\n",
       "      <th>Year of Release</th>\n",
       "      <th>Watch Time</th>\n",
       "      <th>Movie Rating</th>\n",
       "      <th>Metascore of movie</th>\n",
       "      <th>Gross</th>\n",
       "      <th>Votes</th>\n",
       "      <th>Description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>The Shawshank Redemption</td>\n",
       "      <td>1994</td>\n",
       "      <td>142</td>\n",
       "      <td>9.3</td>\n",
       "      <td>82.0</td>\n",
       "      <td>28.34</td>\n",
       "      <td>27,77,378</td>\n",
       "      <td>Over the course of several years, two convicts...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>The Godfather</td>\n",
       "      <td>1972</td>\n",
       "      <td>175</td>\n",
       "      <td>9.2</td>\n",
       "      <td>100.0</td>\n",
       "      <td>134.97</td>\n",
       "      <td>19,33,588</td>\n",
       "      <td>Don Vito Corleone, head of a mafia family, dec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>The Dark Knight</td>\n",
       "      <td>2008</td>\n",
       "      <td>152</td>\n",
       "      <td>9.0</td>\n",
       "      <td>84.0</td>\n",
       "      <td>534.86</td>\n",
       "      <td>27,54,087</td>\n",
       "      <td>When the menace known as the Joker wreaks havo...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                Movie Name Year of Release  Watch Time  \\\n",
       "0           0  The Shawshank Redemption            1994         142   \n",
       "1           1             The Godfather            1972         175   \n",
       "2           2           The Dark Knight            2008         152   \n",
       "\n",
       "   Movie Rating  Metascore of movie   Gross      Votes  \\\n",
       "0           9.3                82.0   28.34  27,77,378   \n",
       "1           9.2               100.0  134.97  19,33,588   \n",
       "2           9.0                84.0  534.86  27,54,087   \n",
       "\n",
       "                                         Description  \n",
       "0  Over the course of several years, two convicts...  \n",
       "1  Don Vito Corleone, head of a mafia family, dec...  \n",
       "2  When the menace known as the Joker wreaks havo...  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BASE_DIR = Path().resolve().parent\n",
    "DATA_DIR = BASE_DIR / 'Data' \n",
    "\n",
    "df = pd.read_csv(DATA_DIR / 'Top_1000_IMDb_movies.csv')\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "05b10dda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['unnamed:_0', 'movie_name', 'year_of_release', 'watch_time',\n",
       "       'movie_rating', 'metascore_of_movie', 'gross', 'votes', 'description'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns = df.columns.str.lower().str.replace(' ', '_')\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "736cad41",
   "metadata": {},
   "source": [
    "## **VADER**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7043e188",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Description'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\NAZIFOU\\Sentiment-Analysis-Top_1000_IMDb_movies\\tf-env\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3804\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3805\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3806\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32mindex.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mindex.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7081\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7089\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'Description'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 26\u001b[0m\n\u001b[0;32m     24\u001b[0m     stemSentence \u001b[38;5;241m=\u001b[39m stemSentence\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[0;32m     25\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m stemSentence\n\u001b[1;32m---> 26\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDescription\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mDescription\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mstr\u001b[39m)\n\u001b[0;32m     27\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDescription\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDescription\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: remove_stopwords(x))\n\u001b[0;32m     28\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDescription\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDescription\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x:clean_text(x))\n",
      "File \u001b[1;32mc:\\Users\\NAZIFOU\\Sentiment-Analysis-Top_1000_IMDb_movies\\tf-env\\lib\\site-packages\\pandas\\core\\frame.py:4102\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   4100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   4101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 4102\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   4104\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[1;32mc:\\Users\\NAZIFOU\\Sentiment-Analysis-Top_1000_IMDb_movies\\tf-env\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3807\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[0;32m   3808\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[0;32m   3809\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[0;32m   3810\u001b[0m     ):\n\u001b[0;32m   3811\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[1;32m-> 3812\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   3814\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3815\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3816\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3817\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'Description'"
     ]
    }
   ],
   "source": [
    "#Remove Stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    " \n",
    "# function to remove stopwords\n",
    "def remove_stopwords(text):\n",
    "    no_stopword_text = [w for w in text.split() if not w in stop_words]\n",
    "    return ' '.join(no_stopword_text)\n",
    "\n",
    "#Clean Text\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(\"[^a-zA-Z]\",\" \",text) \n",
    "    text = ' '.join(text.split()) \n",
    "    return text\n",
    "\n",
    "#stemming\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "def stemming(sentence):\n",
    "    stemSentence = \"\"\n",
    "    for word in sentence.split():\n",
    "        stem = stemmer.stem(word)\n",
    "        stemSentence += stem\n",
    "        stemSentence += \" \"\n",
    "    stemSentence = stemSentence.strip()\n",
    "    return stemSentence\n",
    "df['description'] = df['description'].astype(str)\n",
    "df['description'] = df['description'].apply(lambda x: remove_stopwords(x))\n",
    "df['description'] = df['description'].apply(lambda x:clean_text(x))\n",
    "df['description'] = df['description'].apply(stemming)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4518557",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "# Initialize VADER sentiment analyzer\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Calculate sentiment scores\n",
    "df['VADER_Sentiment'] = df['Description'].apply(lambda text: sia.polarity_scores(text)['compound'])\n",
    "\n",
    "# Categorize sentiment\n",
    "df['VADER_Sentiment_Label'] = df['VADER_Sentiment'].apply(lambda score: 'Positive' if score > 0 else ('Negative' if score < 0 else 'Neutral'))\n",
    "\n",
    "# Print sample results\n",
    "print(df[['Description', 'VADER_Sentiment_Label']].sample(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3219b029",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# Diviser l'ensemble de données en ensembles de formation et de test\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['description'], df['VADER_Sentiment_Label'], test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "607f15a2",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 10\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Tokenize and pad sequences\u001b[39;00m\n\u001b[0;32m      9\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m Tokenizer(num_words\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5000\u001b[39m, oov_token\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m<OOV>\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 10\u001b[0m tokenizer\u001b[38;5;241m.\u001b[39mfit_on_texts(\u001b[43mX_train\u001b[49m)\n\u001b[0;32m     11\u001b[0m X_train_sequences \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mtexts_to_sequences(X_train)\n\u001b[0;32m     12\u001b[0m X_test_sequences \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mtexts_to_sequences(X_test)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'X_train' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "# Tokenize and pad sequences\n",
    "tokenizer = Tokenizer(num_words=5000, oov_token='<OOV>')\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "X_train_sequences = tokenizer.texts_to_sequences(X_train)\n",
    "X_test_sequences = tokenizer.texts_to_sequences(X_test)\n",
    "X_train_padded = pad_sequences(X_train_sequences, maxlen=200, padding='post', truncating='post')\n",
    "X_test_padded = pad_sequences(X_test_sequences, maxlen=200, padding='post', truncating='post')\n",
    "\n",
    "# Build LSTM model\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=5000, output_dim=128, input_length=200))\n",
    "model.add(LSTM(128))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(3, activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Convert sentiment labels to categorical\n",
    "y_train_categorical = pd.get_dummies(y_train)\n",
    "y_test_categorical = pd.get_dummies(y_test)\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train_padded, y_train_categorical, validation_data=(X_test_padded, y_test_categorical), epochs=10, batch_size=32)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
